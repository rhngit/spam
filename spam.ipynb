{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to train a model to detect spam from a set of email features. These features have been predetermined and assumed to be logical, correct, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# read in dataset\n",
    "data = np.genfromtxt('data/spambase/spambase.data', delimiter=',')\n",
    "data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get column names from names file to understand the features\n",
    "f = open('data/spambase/spambase.names', 'r')\n",
    "lines = [line.strip() for line in f]\n",
    "f.close()\n",
    "# comment lines start with | or 1 in this case + empty lines; colnames and type are separated by :\n",
    "colnames = [line.partition(':')[0] for line in lines if not (len(line) == 0 or line[0] == '|' or line[0] == '1')]\n",
    "# need to add the name for the final column\n",
    "colnames.append('spam')\n",
    "len(colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store as DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0            0        0.778        0.000        0.000   \n",
       "1            0        0.372        0.180        0.048   \n",
       "2            0        0.276        0.184        0.010   \n",
       "3            0        0.137        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now there a nicer view of the data, easier to explore\n",
    "df.ix[:3,50:58]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is read in and ready for use, we need to prepare it for training and testing. We will use 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will move the file handling functionality into a dedicated method. This will make running several iterations of the individual algorithms much easier (cross-validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are may types of algorithms to learn the features. We will pick Support Vector Machine (SVM) and Naive Bayes Classifier (NB), as these tend to have a decent performance for this type of dataset and typically do not need much setup.\n",
    "Prior to actually training and validating the models, we also want to perform some form of dimensionality reduction, as we do not know apriori that all features are equally informative or just nuisance parameters. For this reason we compare results using Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA), as well as using all features. For PCA and LDA we will use the extreme case of using just the dominant feature and 10 features.\n",
    "In total we will have (2 Algorithms) x [(2 Dim. Reduction) x (2 feature combinations) + 1] = 10 result sets.\n",
    "\n",
    "As this will be messy in the notebook, there is a helper file (helper.py) that contains relevant classes and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helper import lda, pca, svm, bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a couple of methods to make this process less manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from numpy import mean, var, sum, diag, shape\n",
    "\n",
    "def load_data():\n",
    "    data = genfromtxt('data/spambase/spambase.data', delimiter=',')\n",
    "    target = data[:,-1]\n",
    "    data = data[:,:-1]\n",
    "    return data, target\n",
    "\n",
    "def evaluate(algo, dim_rec, components, iterations=15):\n",
    "    X, y = load_data()\n",
    "    if components > 0:\n",
    "        X, y = dim_rec(X, y, components)\n",
    "    res = []\n",
    "    for i in range(iterations):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        \n",
    "        classifier = algo(X_train, y_train)\n",
    "        confusion = 1.0 * classifier.classify(X_test, y_test) / len(X_test)\n",
    "        res += [confusion]\n",
    "    mean_confusion = mean(res, axis=0)\n",
    "    var_confusion = var(res, axis=0)\n",
    "\n",
    "    return sum(diag(mean_confusion)), iterations * sum(diag(var_confusion)), mean_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Using all components.\n",
      "## svm:\n",
      "### Accuracy of 0.8373507057546146\n",
      "### Error 0.003962395907138027\n",
      "### Confusion Matrix\n",
      "[[ 0.51646761  0.08975751]\n",
      " [ 0.07289178  0.3208831 ]]\n",
      "\n",
      "\n",
      "## bayes:\n",
      "### Accuracy of 0.8243937748823743\n",
      "### Error 0.007406228497168452\n",
      "### Confusion Matrix\n",
      "[[ 0.44618169  0.15888527]\n",
      " [ 0.01672096  0.37821209]]\n",
      "\n",
      "\n",
      "# Using pca\n",
      "# Using 1 components.\n",
      "## svm:\n",
      "### Accuracy of 0.6944625407166125\n",
      "### Error 0.005924731296883787\n",
      "### Confusion Matrix\n",
      "[[ 0.4723127   0.13637351]\n",
      " [ 0.16916395  0.22214984]]\n",
      "\n",
      "\n",
      "## bayes:\n",
      "### Accuracy of 0.6542164314151285\n",
      "### Error 0.002000689269519709\n",
      "### Confusion Matrix\n",
      "[[ 0.58487152  0.02272892]\n",
      " [ 0.32305465  0.06934491]]\n",
      "\n",
      "\n",
      "# Using lda\n",
      "# Using 1 components.\n",
      "## svm:\n",
      "### Accuracy of 0.9109663409337676\n",
      "### Error 0.004394348618690526\n",
      "### Confusion Matrix\n",
      "[[ 0.56728194  0.03923272]\n",
      " [ 0.04980094  0.3436844 ]]\n",
      "\n",
      "\n",
      "## bayes:\n",
      "### Accuracy of 0.8984437205935577\n",
      "### Error 0.005735476906523811\n",
      "### Confusion Matrix\n",
      "[[ 0.57690916  0.02909881]\n",
      " [ 0.07245747  0.32153456]]\n",
      "\n",
      "\n",
      "# Using pca\n",
      "# Using 10 components.\n",
      "## svm:\n",
      "### Accuracy of 0.8014477017734347\n",
      "### Error 0.016226206152889725\n",
      "### Confusion Matrix\n",
      "[[ 0.50503076  0.10025335]\n",
      " [ 0.09829895  0.29641694]]\n",
      "\n",
      "\n",
      "## bayes:\n",
      "### Accuracy of 0.6728193992037642\n",
      "### Error 0.004899393765058115\n",
      "### Confusion Matrix\n",
      "[[ 0.58733261  0.01838581]\n",
      " [ 0.30879479  0.08548679]]\n",
      "\n",
      "\n",
      "# Using lda\n",
      "# Using 10 components.\n",
      "## svm:\n",
      "### Accuracy of 0.9130655085052479\n",
      "### Error 0.006490451023549525\n",
      "### Confusion Matrix\n",
      "[[ 0.56293883  0.04010134]\n",
      " [ 0.04683315  0.35012667]]\n",
      "\n",
      "\n",
      "## bayes:\n",
      "### Accuracy of 0.8942453854505973\n",
      "### Error 0.005786405828846596\n",
      "### Confusion Matrix\n",
      "[[ 0.57032211  0.03279045]\n",
      " [ 0.07296417  0.32392327]]\n",
      "\n",
      "\n",
      "##############################\n",
      "Best performing combination\n",
      "Algorithm: svm\n",
      "Components: 10\n",
      "Decomposition: lda\n",
      "Accuracy: 0.913066\n",
      "Error: 0.006490\n"
     ]
    }
   ],
   "source": [
    "components = [0,1,10] # 0 will be handled like 'all'\n",
    "dim_rec = [pca, lda]\n",
    "algo = [svm, bayes]\n",
    "best = [0.0, 0.0, 'none', 'none', 0]\n",
    "acc, err, mat = 0.0, 0.0, 0.0\n",
    "for c in components:\n",
    "    for d in dim_rec:\n",
    "        tmp = 'all'\n",
    "        if(c>0):\n",
    "            tmp = str(c)\n",
    "            print (\"# Using %s\" % (d.__name__))\n",
    "\n",
    "        print (\"# Using %s components.\" % (tmp))\n",
    "        for a in algo:\n",
    "            print (\"## %s:\" % a.__name__)\n",
    "            acc, err, mat = evaluate(a, d, c)\n",
    "            \n",
    "            print (\"### Accuracy of {n}\".format(n=acc))\n",
    "            print (\"### Error {n}\".format(n=err))\n",
    "            print (\"### Confusion Matrix\")\n",
    "            print (mat)\n",
    "            print ()\n",
    "            if acc > best[0]:\n",
    "                best = [acc, err, d.__name__, a.__name__, c]\n",
    "            print ()\n",
    "        if(c==0):\n",
    "            break\n",
    "\n",
    "print (\"##############################\")            \n",
    "print (\"  Best performing combination\")\n",
    "print (\"  Algorithm: %s\" % (best[3]))\n",
    "print (\"  Components: %s\" % (best[4])) \n",
    "print (\"  Decomposition: %s\" % (best[2])) \n",
    "print (\"  Accuracy: %f\" % (best[0]))   \n",
    "print (\"  Error: %f\" % (best[1]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
